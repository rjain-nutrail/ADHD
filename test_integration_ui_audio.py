"""
Integration tests for UI and audio pipeline

This module contains integration tests that verify the Gradio interface modifications,
assessment flow, webcam continuity during assessment, and text-to-speech integration
with question presentation.

Requirements tested:
- 2.1: Webcam interface remains interactive during assessment
- 2.2: Webcam frame updates at minimum 15 FPS during assessment  
- 4.1: Assessment integrates with chat interface
"""

import unittest
import time
import threading
import tempfile
import shutil
import os
from unittest.mock import Mock, patch, MagicMock, call
import gradio as gr

# Import the modules we're testing
from interactive_assessment import InteractiveADHDAssessment, AssessmentState
import main


class TestGradioInterfaceIntegration(unittest.TestCase):
    """Test Gradio interface modifications and assessment flow"""
    
    def setUp(self):
        """Set up test fixtures for UI integration tests"""
        self.temp_dir = tempfile.mkdtemp()
        
        # Mock the global assessment system
        self.mock_assessment = Mock()
        self.mock_assessment.state = AssessmentState.NOT_STARTED
        self.mock_assessment.get_current_state.return_value = {
            'state': 'not_started',
            'current_question_index': 0,
            'total_questions': 5,
            'interruption_count': 0,
            'is_presenting_question': False,
            'progress_percentage': 0.0
        }
        self.mock_assessment.ASSESSMENT_QUESTIONS = [
            "Tell me about your typical morning routine. How do you usually start your day?"
        ]
        self.mock_assessment.start_assessment.return_value = True
        self.mock_assessment.recover_from_error.return_value = True
        self.mock_assessment.set_callbacks.return_value = None
        self.mock_assessment.list_available_sessions.return_value = []
        self.mock_assessment.reset_assessment.return_value = None
        self.mock_assessment.format_results_summary.return_value = "Assessment COMPLETED | ADHD Indicators: NOT DETECTED | Interruptions: 0/3"
        self.mock_assessment.format_results_for_display.return_value = "ADHD ASSESSMENT RESULTS\nNo ADHD indicators detected"
        
        # Patch the global assessment system in main module
        self.assessment_patcher = patch('main.assessment_system', self.mock_assessment)
        self.assessment_patcher.start()
        
        # Mock global state variables
        self.globals_patcher = patch.multiple(
            'main',
            assessment_active=False,
            assessment_results=None
        )
        self.globals_patcher.start()
    
    def tearDown(self):
        """Clean up after UI integration tests"""
        self.assessment_patcher.stop()
        self.globals_patcher.stop()
        
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
    
    @patch('main.text_to_speech_with_elevenlabs')
    def test_start_adhd_assessment_ui_integration(self, mock_tts):
        """Test ADHD assessment start through UI interface - Requirements 4.1"""
        mock_tts.return_value = None
        
        # Configure mock assessment to simulate successful start
        self.mock_assessment.start_assessment.return_value = True
        self.mock_assessment.get_current_state.return_value = {
            'state': 'in_progress',
            'current_question_index': 0,
            'total_questions': 5,
            'interruption_count': 0,
            'is_presenting_question': True,
            'progress_percentage': 0.0
        }
        self.mock_assessment.ASSESSMENT_QUESTIONS = [
            "Tell me about your typical morning routine. How do you usually start your day?"
        ]
        
        # Test the start_adhd_assessment function
        result = main.start_adhd_assessment()
        
        # Verify function returns expected UI components
        self.assertIsInstance(result, tuple)
        self.assertEqual(len(result), 3)  # chat_history, audio_output, status_text
        
        chat_history, audio_output, status_text = result
        
        # Verify chat history contains assessment start message
        self.assertIsInstance(chat_history, list)
        self.assertTrue(len(chat_history) > 0)
        self.assertIn("ADHD Assessment Started", chat_history[0][1])
        
        # Verify audio output path is provided
        self.assertEqual(audio_output, "assessment_audio.wav")
        
        # Verify status text indicates assessment started
        self.assertIn("Assessment started", status_text)
        self.assertIn("Question 1/5", status_text)
        
        # Verify assessment system was called correctly
        self.mock_assessment.start_assessment.assert_called_once()
        self.mock_assessment.set_callbacks.assert_called()
    
    def test_assessment_progress_ui_updates(self):
        """Test assessment progress updates in UI - Requirements 4.1"""
        # Test progress when no assessment is active
        progress = main.get_assessment_progress()
        self.assertEqual(progress, "No assessment in progress")
        
        # Test progress during active assessment
        with patch('main.assessment_active', True):
            self.mock_assessment.get_current_state.return_value = {
                'state': 'in_progress',
                'current_question_index': 2,
                'total_questions': 5,
                'interruption_count': 1,
                'is_presenting_question': True,
                'progress_percentage': 60.0
            }
            self.mock_assessment.is_presenting_question = True
            
            progress = main.get_assessment_progress()
            
            # Verify progress contains expected information
            self.assertIn("Question 3/5", progress)
            self.assertIn("60%", progress)
            self.assertIn("Presenting question", progress)
            self.assertIn("Interruptions: 1", progress)
            self.assertIn("üé§", progress)  # Presentation icon
        
        # Test progress when waiting for response
        self.mock_assessment.is_presenting_question = False
        with patch('main.assessment_active', True):
            progress = main.get_assessment_progress()
            self.assertIn("Waiting for response", progress)
            self.assertIn("‚è≥", progress)  # Waiting icon
    
    @patch('main.text_to_speech_with_elevenlabs')
    def test_assessment_recovery_ui_integration(self, mock_tts):
        """Test assessment recovery through UI interface"""
        mock_tts.return_value = None
        
        # Configure mock for recovery scenario
        self.mock_assessment.state = AssessmentState.ERROR
        self.mock_assessment.recover_from_error.return_value = True
        self.mock_assessment.get_current_state.return_value = {
            'state': 'in_progress',
            'current_question_index': 2,
            'total_questions': 5,
            'interruption_count': 1,
            'is_presenting_question': False,
            'progress_percentage': 40.0
        }
        
        # Test recovery function
        result = main.recover_assessment()
        
        # Verify recovery result structure
        self.assertIsInstance(result, tuple)
        self.assertEqual(len(result), 3)
        
        chat_history, audio_output, status_text = result
        
        # Verify recovery success message
        self.assertIn("Assessment recovered successfully", chat_history[0][1])
        self.assertIn("Continuing from question", chat_history[0][1])
        
        # Verify recovery was attempted
        self.mock_assessment.recover_from_error.assert_called_once()
    
    def test_assessment_results_display_ui(self):
        """Test assessment results display in UI components"""
        # Mock results data
        mock_results = {
            'adhd_flagged': True,
            'interruption_count': 4,
            'interruption_threshold': 3,
            'assessment_completed': True,
            'assessment_message': 'ADHD indicators detected - 4 interruptions recorded',
            'disclaimer': 'This assessment is not diagnostic.',
            'assessment_datetime': '2024-01-01T12:00:00',
            'assessment_id': 'test_assessment_123'
        }
        
        # Mock the assessment system methods
        self.mock_assessment.format_results_summary.return_value = "Assessment COMPLETED | ADHD Indicators: DETECTED | Interruptions: 4/3"
        self.mock_assessment.format_results_for_display.return_value = "ADHD ASSESSMENT RESULTS\nADHD INDICATORS DETECTED\n4 interruptions recorded"
        
        with patch('main.assessment_results', mock_results):
            # Test results summary
            summary = main.get_assessment_results_summary()
            self.assertIn("ADHD Indicators: DETECTED", summary)
            self.assertIn("Interruptions: 4/3", summary)
            
            # Test results display
            display = main.get_assessment_results_display()
            self.assertIn("ADHD ASSESSMENT RESULTS", display)
            self.assertIn("ADHD INDICATORS DETECTED", display)
            self.assertIn("4 interruptions", display)
    
    def test_chat_interface_assessment_mode_integration(self):
        """Test chat interface behavior during assessment mode - Requirements 4.1"""
        # Test normal chat mode (no assessment active)
        with patch('main.assessment_active', False):
            # This would normally be tested through the speech recognition callback
            # but we can test the logic that determines assessment vs normal mode
            self.assertFalse(main.assessment_active)
        
        # Test assessment mode behavior
        with patch('main.assessment_active', True):
            self.mock_assessment.state = AssessmentState.IN_PROGRESS
            self.mock_assessment.is_presenting_question = True
            
            # Update mock to return in_progress state
            self.mock_assessment.get_current_state.return_value = {
                'state': 'in_progress',
                'current_question_index': 0,
                'total_questions': 5,
                'interruption_count': 0,
                'is_presenting_question': True,
                'progress_percentage': 0.0
            }
            
            # Verify assessment mode is active
            self.assertTrue(main.assessment_active)
            
            # Test that assessment state affects UI behavior
            state = self.mock_assessment.get_current_state()
            self.assertEqual(state['state'], 'in_progress')


class TestWebcamContinuityDuringAssessment(unittest.TestCase):
    """Test webcam continuity during assessment - Requirements 2.1, 2.2"""
    
    def setUp(self):
        """Set up test fixtures for webcam tests"""
        # Mock cv2 and camera operations
        self.cv2_patcher = patch('main.cv2')
        self.mock_cv2 = self.cv2_patcher.start()
        
        # Mock camera object
        self.mock_camera = Mock()
        self.mock_camera.isOpened.return_value = True
        self.mock_camera.read.return_value = (True, Mock())  # (success, frame)
        self.mock_camera.get.return_value = 1  # Buffer size
        
        # Patch global camera variable
        self.camera_patcher = patch('main.camera', self.mock_camera)
        self.camera_patcher.start()
        
        # Mock assessment system
        self.mock_assessment = Mock(spec=InteractiveADHDAssessment)
        self.assessment_patcher = patch('main.assessment_system', self.mock_assessment)
        self.assessment_patcher.start()
    
    def tearDown(self):
        """Clean up after webcam tests"""
        self.cv2_patcher.stop()
        self.camera_patcher.stop()
        self.assessment_patcher.stop()
    
    def test_webcam_initialization_during_assessment(self):
        """Test webcam initialization works during assessment"""
        # Mock cv2 constants
        self.mock_cv2.CAP_PROP_FRAME_WIDTH = 3
        self.mock_cv2.CAP_PROP_FRAME_HEIGHT = 4
        self.mock_cv2.CAP_PROP_FPS = 5
        self.mock_cv2.CAP_PROP_BUFFERSIZE = 38
        
        # Test camera initialization
        with patch('main.camera', None):  # Ensure camera is None initially
            with patch('main.cv2.VideoCapture', return_value=self.mock_camera):
                result = main.initialize_camera()
        
        # Verify camera was configured with optimized settings
        self.assertTrue(result)
        # Verify set was called (exact values may vary based on cv2 constants)
        self.assertGreater(self.mock_camera.set.call_count, 0)
    
    def test_webcam_frame_updates_during_assessment(self):
        """Test webcam frame updates continue during assessment - Requirements 2.2"""
        # Mock frame data
        mock_frame = Mock()
        self.mock_camera.read.return_value = (True, mock_frame)
        self.mock_cv2.cvtColor.return_value = mock_frame
        
        # Start webcam
        with patch('main.is_running', True):
            frame = main.get_webcam_frame()
            
            # Verify frame was retrieved and processed
            self.mock_camera.read.assert_called()
            self.mock_cv2.cvtColor.assert_called_with(mock_frame, main.cv2.COLOR_BGR2RGB)
            self.assertEqual(frame, mock_frame)
    
    def test_webcam_performance_optimization(self):
        """Test webcam performance optimizations for smooth operation"""
        # Test buffer management for reduced lag
        self.mock_camera.get.return_value = 3  # Multiple frames in buffer
        mock_frame = Mock()
        self.mock_camera.read.return_value = (True, mock_frame)
        
        with patch('main.is_running', True):
            main.get_webcam_frame()
            
            # Verify buffer was cleared (multiple read calls to clear buffer)
            self.assertGreaterEqual(self.mock_camera.read.call_count, 2)
    
    def test_webcam_continues_during_assessment_state_changes(self):
        """Test webcam continues running through assessment state changes"""
        # Test webcam during different assessment states
        assessment_states = [
            AssessmentState.NOT_STARTED,
            AssessmentState.IN_PROGRESS,
            AssessmentState.PAUSED,
            AssessmentState.COMPLETED
        ]
        
        for state in assessment_states:
            with self.subTest(state=state):
                self.mock_assessment.state = state
                
                # Mock successful frame capture
                mock_frame = Mock()
                self.mock_camera.read.return_value = (True, mock_frame)
                self.mock_cv2.cvtColor.return_value = mock_frame
                
                with patch('main.is_running', True):
                    frame = main.get_webcam_frame()
                    
                    # Verify webcam continues working regardless of assessment state
                    self.assertIsNotNone(frame)
                    self.mock_camera.read.assert_called()
    
    def test_webcam_error_handling_during_assessment(self):
        """Test webcam error handling doesn't interrupt assessment"""
        # Test camera read failure
        self.mock_camera.read.return_value = (False, None)
        
        with patch('main.is_running', True), patch('main.last_frame', Mock()) as mock_last_frame:
            frame = main.get_webcam_frame()
            
            # Verify last frame is returned when read fails
            self.assertEqual(frame, mock_last_frame)
    
    def test_ensure_webcam_during_assessment_function(self):
        """Test the ensure_webcam_during_assessment function"""
        # Mock get_webcam_frame
        mock_frame = Mock()
        with patch('main.get_webcam_frame', return_value=mock_frame) as mock_get_frame:
            result = main.ensure_webcam_during_assessment()
            
            # Verify function calls get_webcam_frame and returns result
            mock_get_frame.assert_called_once()
            self.assertEqual(result, mock_frame)


class TestTextToSpeechIntegration(unittest.TestCase):
    """Test text-to-speech integration with question presentation"""
    
    def setUp(self):
        """Set up test fixtures for TTS integration tests"""
        self.temp_dir = tempfile.mkdtemp()
        
        # Create test assessment instance
        self.assessment = InteractiveADHDAssessment(
            question_timeout=2,
            audio_output_path=os.path.join(self.temp_dir, "test_audio.wav"),
            results_storage_path=os.path.join(self.temp_dir, "results"),
            session_storage_path=os.path.join(self.temp_dir, "sessions")
        )
        
        # Mock TTS function
        self.tts_patcher = patch('interactive_assessment.text_to_speech_with_elevenlabs')
        self.mock_tts = self.tts_patcher.start()
        self.mock_tts.return_value = None
    
    def tearDown(self):
        """Clean up after TTS integration tests"""
        self.tts_patcher.stop()
        self.assessment.reset_assessment()
        
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
    
    def test_tts_integration_with_question_presentation(self):
        """Test TTS integration during question presentation"""
        # Start assessment
        success = self.assessment.start_assessment()
        self.assertTrue(success)
        
        # Wait for presentation thread to complete
        if self.assessment.presentation_thread:
            self.assessment.presentation_thread.join(timeout=5)
        
        # Verify TTS was called with the first question
        self.mock_tts.assert_called_once()
        call_args = self.mock_tts.call_args
        
        # Check that TTS was called with correct question text and audio path
        self.assertEqual(call_args[0][0], self.assessment.ASSESSMENT_QUESTIONS[0])
        self.assertEqual(call_args[0][1], self.assessment.audio_output_path)
    
    def test_tts_timing_integration_with_assessment(self):
        """Test TTS timing integration with assessment timing system"""
        # Mock TTS to take specific time
        def mock_tts_with_delay(*args, **kwargs):
            time.sleep(0.5)  # Simulate 0.5 second TTS duration
        
        self.mock_tts.side_effect = mock_tts_with_delay
        
        # Start assessment and measure timing
        start_time = time.time()
        self.assessment.start_assessment()
        
        # Wait for presentation to complete
        if self.assessment.presentation_thread:
            self.assessment.presentation_thread.join(timeout=3)
        
        # Verify timing was recorded correctly
        self.assertEqual(len(self.assessment.responses), 1)
        response = self.assessment.responses[0]
        
        self.assertIsNotNone(response.start_time)
        self.assertIsNotNone(response.completion_time)
        self.assertIsNotNone(response.presentation_duration)
        
        # Verify timing accuracy (should be around 0.5 seconds)
        self.assertGreater(response.presentation_duration, 0.4)
        self.assertLess(response.presentation_duration, 0.6)
    
    def test_tts_error_handling_integration(self):
        """Test TTS error handling integration with assessment system"""
        # Mock TTS to raise an exception
        self.mock_tts.side_effect = Exception("TTS system failure")
        
        # Start assessment
        success = self.assessment.start_assessment()
        self.assertTrue(success)  # Assessment should start despite TTS failure
        
        # Wait for presentation thread
        if self.assessment.presentation_thread:
            self.assessment.presentation_thread.join(timeout=3)
        
        # Verify assessment continues despite TTS failure
        self.assertEqual(len(self.assessment.responses), 1)
        response = self.assessment.responses[0]
        
        # Question should still be marked as presented (start time should exist)
        self.assertIsNotNone(response.start_time)
        # Note: completion_time might be None due to error handling, which is acceptable
    
    def test_tts_integration_across_multiple_questions(self):
        """Test TTS integration across multiple questions in sequence"""
        # Start assessment
        self.assessment.start_assessment()
        
        # Wait for first question
        if self.assessment.presentation_thread:
            self.assessment.presentation_thread.join(timeout=3)
        
        # Simulate user response to proceed to next question
        self.assessment.handle_user_response("My response to question 1")
        
        # Wait for second question
        if self.assessment.presentation_thread:
            self.assessment.presentation_thread.join(timeout=3)
        
        # Verify TTS was called for both questions
        self.assertEqual(self.mock_tts.call_count, 2)
        
        # Verify correct questions were presented
        call_args_list = self.mock_tts.call_args_list
        self.assertEqual(call_args_list[0][0][0], self.assessment.ASSESSMENT_QUESTIONS[0])
        self.assertEqual(call_args_list[1][0][0], self.assessment.ASSESSMENT_QUESTIONS[1])
    
    def test_speech_recognition_integration_setup(self):
        """Test speech recognition integration setup with assessment"""
        # Mock speech recognizer
        mock_speech_recognizer = Mock()
        
        with patch('main.speech_recognizer', mock_speech_recognizer):
            # Test the setup function
            main.setup_assessment_speech_integration()
            
            # Verify callbacks were configured on assessment system
            # This tests that the integration setup function works
            self.assertTrue(callable(main.setup_assessment_speech_integration))
    
    def test_audio_pipeline_integration_with_ui(self):
        """Test complete audio pipeline integration with UI components"""
        # Mock the main audio processing function components
        with patch('main.speech_recognizer') as mock_recognizer, \
             patch('main.adhd_detector') as mock_detector, \
             patch('main.ask_agent') as mock_agent:
            
            mock_agent.return_value = "Test response"
            mock_detector.get_adhd_status.return_value = {"flagged": False}
            
            # Test that audio pipeline components are properly integrated
            # This verifies the overall integration structure exists
            self.assertTrue(hasattr(main, 'process_audio_and_chat'))
            self.assertTrue(hasattr(main, 'setup_assessment_speech_integration'))


class TestAssessmentFlowIntegration(unittest.TestCase):
    """Test complete assessment flow integration"""
    
    def setUp(self):
        """Set up test fixtures for flow integration tests"""
        self.temp_dir = tempfile.mkdtemp()
        
        # Mock TTS to avoid actual audio generation
        self.tts_patcher = patch('interactive_assessment.text_to_speech_with_elevenlabs')
        self.mock_tts = self.tts_patcher.start()
        self.mock_tts.return_value = None
        
        # Create assessment instance
        self.assessment = InteractiveADHDAssessment(
            question_timeout=1,  # Short timeout for testing
            audio_output_path=os.path.join(self.temp_dir, "test_audio.wav"),
            results_storage_path=os.path.join(self.temp_dir, "results"),
            session_storage_path=os.path.join(self.temp_dir, "sessions")
        )
    
    def tearDown(self):
        """Clean up after flow integration tests"""
        self.tts_patcher.stop()
        self.assessment.reset_assessment()
        
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
    
    def test_complete_assessment_flow_integration(self):
        """Test complete assessment flow from start to finish"""
        # Track callback calls
        callback_calls = {
            'question_start': [],
            'question_complete': [],
            'interruption': [],
            'assessment_complete': []
        }
        
        def on_question_start(q_idx, q_text):
            callback_calls['question_start'].append((q_idx, q_text))
        
        def on_question_complete(q_idx):
            callback_calls['question_complete'].append(q_idx)
        
        def on_interruption(count, timestamp):
            callback_calls['interruption'].append((count, timestamp))
        
        def on_assessment_complete(results):
            callback_calls['assessment_complete'].append(results)
        
        # Set up callbacks
        self.assessment.set_callbacks(
            on_question_start=on_question_start,
            on_question_complete=on_question_complete,
            on_interruption=on_interruption,
            on_assessment_complete=on_assessment_complete
        )
        
        # Start assessment
        success = self.assessment.start_assessment()
        self.assertTrue(success)
        
        # Process through all questions with some interruptions
        for i in range(5):  # 5 questions total
            # Wait for question presentation
            if self.assessment.presentation_thread:
                self.assessment.presentation_thread.join(timeout=3)
            
            # Simulate interruption on first 3 questions
            if i < 3:
                # Simulate interruption during presentation
                self.assessment.is_presenting_question = True
                self.assessment.handle_user_response(f"Interrupting question {i+1}!")
            else:
                # Normal response after question completion
                self.assessment.handle_user_response(f"Normal response to question {i+1}")
        
        # Verify complete flow
        self.assertEqual(self.assessment.state, AssessmentState.COMPLETED)
        self.assertEqual(len(callback_calls['question_start']), 5)
        self.assertEqual(len(callback_calls['question_complete']), 5)
        self.assertEqual(len(callback_calls['interruption']), 3)
        self.assertEqual(len(callback_calls['assessment_complete']), 1)
        
        # Verify results
        results = callback_calls['assessment_complete'][0]
        self.assertTrue(results['adhd_flagged'])  # 3 interruptions >= threshold
        self.assertEqual(results['interruption_count'], 3)
    
    def test_assessment_state_transitions_integration(self):
        """Test assessment state transitions through complete flow"""
        # Track state changes
        states_observed = []
        
        def track_state():
            states_observed.append(self.assessment.state)
        
        # Initial state
        track_state()
        self.assertEqual(self.assessment.state, AssessmentState.NOT_STARTED)
        
        # Start assessment
        self.assessment.start_assessment()
        track_state()
        self.assertEqual(self.assessment.state, AssessmentState.IN_PROGRESS)
        
        # Complete all questions quickly
        for i in range(5):
            if self.assessment.presentation_thread:
                self.assessment.presentation_thread.join(timeout=3)
            self.assessment.handle_user_response(f"Response {i+1}")
        
        track_state()
        self.assertEqual(self.assessment.state, AssessmentState.COMPLETED)
        
        # Verify state progression
        expected_states = [
            AssessmentState.NOT_STARTED,
            AssessmentState.IN_PROGRESS,
            AssessmentState.COMPLETED
        ]
        self.assertEqual(states_observed, expected_states)


if __name__ == '__main__':
    # Configure logging for tests
    import logging
    logging.basicConfig(level=logging.WARNING)  # Reduce log noise during tests
    
    # Create test suite
    test_suite = unittest.TestSuite()
    
    # Add integration test cases
    test_suite.addTest(unittest.makeSuite(TestGradioInterfaceIntegration))
    test_suite.addTest(unittest.makeSuite(TestWebcamContinuityDuringAssessment))
    test_suite.addTest(unittest.makeSuite(TestTextToSpeechIntegration))
    test_suite.addTest(unittest.makeSuite(TestAssessmentFlowIntegration))
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    # Print summary
    print(f"\nIntegration Test Summary:")
    print(f"Tests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    
    if result.failures:
        print(f"\nFailures:")
        for test, traceback in result.failures:
            print(f"- {test}: {traceback.split('AssertionError:')[-1].strip()}")
    
    if result.errors:
        print(f"\nErrors:")
        for test, traceback in result.errors:
            print(f"- {test}: {traceback.split('Error:')[-1].strip()}")
    
    success_rate = ((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100) if result.testsRun > 0 else 0
    print(f"Success rate: {success_rate:.1f}%")
    
    # Exit with appropriate code
    exit_code = 0 if len(result.failures) == 0 and len(result.errors) == 0 else 1
    exit(exit_code)